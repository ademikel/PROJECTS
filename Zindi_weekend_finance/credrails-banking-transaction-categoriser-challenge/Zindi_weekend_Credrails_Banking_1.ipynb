{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VlUoA_0IV7pq"
   },
   "outputs": [],
   "source": [
    "# !pip -q install git+https://github.com/eaedk/testing-zindi-package.git\n",
    "# from zindi.user import Zindian\n",
    "# USERNAME = \"adetoromichael346@gmail.com\" #@param {type : \"string\"}\n",
    "# user = Zindian(username = USERNAME)\n",
    "# user.select_a_challenge(reward = 'all', kind = 'hackathon', active = 'true')\n",
    "# user.download_dataset(destination = \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g3BDjy_IWIBS"
   },
   "outputs": [],
   "source": [
    "# !pip install catboost optuna # installing catboost and optuna libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DNdGUY8BWo3v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MIKE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import string\n",
    "import datetime\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# import optuna\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NIzVhQVbziyx"
   },
   "outputs": [],
   "source": [
    "#Reduce Memory Usage\n",
    "def reduce_memory_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "    \n",
    "        if col_type != 'object':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SW3rEc70W06Q"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "sub = pd.read_csv('SampleSubmission.csv')\n",
    "random_seed = 101 # random seed for all computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "KGeSuiUWXEe7",
    "outputId": "3b3ecd19-92cf-4d17-ec24-c5799c475dc1"
   },
   "outputs": [],
   "source": [
    "# display(train.head(), test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57974, 8), (22625, 8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['DATE'] = pd.to_datetime(train['DATE'], format='%Y-%m-%d')\n",
    "# test['DATE'] = pd.to_datetime(test['DATE'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values('DATE').reset_index(drop=True) \n",
    "test = test.sort_values('DATE').reset_index(drop=True)\n",
    "\n",
    "# for dataset in (train, test):\n",
    "#     dataset['Day'] = pd.to_datetime(dataset['DATE']).dt.day\n",
    "#     # dataset['Day'] = dataset.DATE.dt.day\n",
    "#     # dataset['Month'] = dataset.DATE.dt.month\n",
    "#     # dataset['Year'] = dataset.DATE.dt.year\n",
    "#     # dataset.set_index('DATE', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R6MMGwAnh0PN"
   },
   "outputs": [],
   "source": [
    "# Hypotheses\n",
    "# Transaction details will have a huge impact in determining which category the transaction belong to.\n",
    "# Date might also not has a great impact on the transaction category, so it will be dropped\n",
    "# Due to behaviour on customers, account no. which symbolizes a customer might has a little impact in determining the category of a transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-pKmmBFNqFDE"
   },
   "outputs": [],
   "source": [
    "# Train['TRANSACTION DETAILS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z7QGvewHjfnR"
   },
   "outputs": [],
   "source": [
    "# display(Train.isna().sum(), Test.isna().sum())\n",
    "# Train['TRANSACTION DETAILS'].unique()\n",
    "# I will drop cheque no. due to presence of huge missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Bzuezoqam8ZS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name, group in train.groupby(train.Category):\n",
    "#     print(name)\n",
    "#     print(group)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "m0QdD3BPXUed"
   },
   "outputs": [],
   "source": [
    "# cols = Train.columns\n",
    "# def card(cols):\n",
    "#   for col in cols:\n",
    "#     print(f'{col}: {Train[col].nunique()}')\n",
    "# card(cols)\n",
    "# print('len of data: ', Train.shape[0])\n",
    "# # the balance amount might not really matter in determining the category and it also has high cardinality, so it will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Aw0n8Z6-YCoD"
   },
   "outputs": [],
   "source": [
    "# #Target Histogram\n",
    "# plt.figure(figsize=(20, 15))\n",
    "# sns.set_style(\"white\") \n",
    "# sns.countplot( y='Category', data=Train, palette=\"Set1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MFIoR1xqZION"
   },
   "outputs": [],
   "source": [
    "# Train.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SgwVwfxNYWpE"
   },
   "outputs": [],
   "source": [
    "# #Numerical Features Histograms (Train)\n",
    "# num_feats = list(Train.select_dtypes(include=['int64', 'float64', 'int32']).columns)\n",
    "# Train[num_feats].hist(figsize=(20,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "g4yQhgFpYxtf"
   },
   "outputs": [],
   "source": [
    "# #Target VS USER_GENDER \n",
    "# sns.countplot( y='Account_NO', data=Train, palette=\"Set1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gyE0rlh_oZIF"
   },
   "outputs": [],
   "source": [
    "# train.iloc[np.where(train['TRANSACTION DETAILS'].isnull() == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(how='all')\n",
    "test = test.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56203, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dropna(subset = ['Category'], inplace = True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2W0Ozg4aqkJW"
   },
   "outputs": [],
   "source": [
    "ID = test.ID\n",
    "test.drop('ID', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(train.isna().sum(), test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YI5wNgB5qXkA"
   },
   "outputs": [],
   "source": [
    "train['TRANSACTION DETAILS'].fillna('NEFT/FDRL401249531/INDIAFORENSIC', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DATE', 'TRANSACTION DETAILS', 'Account_NO', 'CHQ.NO.',\n",
       "       'WITHDRAWAL AMT', 'DEPOSIT AMT', 'BALANCE AMT', 'Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis = 0)\n",
    "# def Agg_1(Feature):\n",
    "#     for dataset in (train, test):\n",
    "#         for key in ['WITHDRAWAL AMT', 'DEPOSIT AMT']:\n",
    "#             dataset[f'{Feature}_{key}_Min'] = dataset[key].map(dict(data.groupby(key)[Feature].min()))\n",
    "#             dataset[f'{Feature}_{key}_Max'] = dataset[key].map(dict(data.groupby(key)[Feature].max()))\n",
    "#             dataset[f'{Feature}_{key}_Mean'] = dataset[key].map(dict(data.groupby(key)[Feature].mean()))\n",
    "            \n",
    "# Agg_1('Account_NO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Agg_2(Feature):\n",
    "    for dataset in (train, test):\n",
    "        for key in ['TRANSACTION DETAILS']:\n",
    "            dataset[f'{Feature}_{key}_Mode'] = dataset[key].map(dict(data.groupby(key)[Feature].apply(lambda x: x.value_counts().keys()[0])))\n",
    "            dataset[f'{Feature}_{key}_Count'] = dataset[key].map(dict(data.groupby(key)[Feature].count()))\n",
    "            dataset[f'{Feature}_{key}_NUnique'] = dataset[key].map(dict(data.groupby(key)[Feature].nunique()))\n",
    "            \n",
    "Agg_2('Account_NO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vMA14LWGrzIg"
   },
   "outputs": [],
   "source": [
    "train['isGreater'] = np.where(train['DEPOSIT AMT'] > train['WITHDRAWAL AMT'], 1, 0)\n",
    "test['isGreater'] = np.where(test['DEPOSIT AMT'] > test['WITHDRAWAL AMT'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DATE', 'TRANSACTION DETAILS', 'Account_NO', 'CHQ.NO.',\n",
       "       'WITHDRAWAL AMT', 'DEPOSIT AMT', 'BALANCE AMT', 'Category',\n",
       "       'Account_NO_TRANSACTION DETAILS_Mode',\n",
       "       'Account_NO_TRANSACTION DETAILS_Count',\n",
       "       'Account_NO_TRANSACTION DETAILS_NUnique', 'isGreater'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZXvd49vmrPGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset in (train, test):\n",
    "    dataset.drop(['CHQ.NO.', 'WITHDRAWAL AMT', 'DEPOSIT AMT', 'BALANCE AMT', 'DATE'], axis = 1, inplace = True)\n",
    "# , 'Account_NO', 'WITHDRAWAL AMT', 'DEPOSIT AMT', 'BALANCE AMT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ivtkL05zKMT",
    "outputId": "2d6b7fc4-2d81-4d2f-a6cf-af28c807e9ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "IJ47ygvDzlSH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reduce_memory_usage(train), reduce_memory_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7u2uLnOsor5_",
    "outputId": "dc085e76-1087-451c-e8b1-fda38ca57670"
   },
   "outputs": [],
   "source": [
    "# data = pd.concat([train, test], sort = False).reset_index(drop = True)\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TRANSACTION DETAILS'] = train['TRANSACTION DETAILS'].str.replace('/', '')\n",
    "test['TRANSACTION DETAILS'] = test['TRANSACTION DETAILS'].str.replace('/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56203, 7), (22625, 6))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(min_df = 4, max_features = 500, analyzer = 'char', stop_words = 'english', ngram_range = (3, 5), dtype = np.float32)\n",
    "# tfidf.fit(train['TRANSACTION DETAILS'])\n",
    "# train_feat = tfidf.transform(train['TRANSACTION DETAILS'])\n",
    "# test_feat = tfidf.transform(test['TRANSACTION DETAILS'])\n",
    "# vec_train = pd.DataFrame(train_feat.todense(), columns = tfidf.get_feature_names())\n",
    "# vec_test = pd.DataFrame(test_feat.todense(), columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([train.reset_index(), vec_train], axis = 1)\n",
    "# test = pd.concat([test.reset_index(), vec_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tag_part_of_speech_noun(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    return noun_count\n",
    "\n",
    "def tag_part_of_speech_adj(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    adjective_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    return adjective_count\n",
    "\n",
    "def tag_part_of_speech_verb(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    verb_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    return verb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for df in ([train, test]):\n",
    "    df['TRANSACTION_DETAILS_Length'] = df['TRANSACTION DETAILS'].apply(lambda x : len(x))\n",
    "    df['Num_Words'] = df['TRANSACTION DETAILS'].apply(lambda comment: len(comment.split()))\n",
    "    df['Num_Unique_Words'] = df['TRANSACTION DETAILS'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['Words_VS_Unique'] = df['Num_Unique_Words'] / df['Num_Words']\n",
    "    df['num_punctuation'] = df['TRANSACTION DETAILS'].apply(lambda comment: sum(comment.count(w) for w in '\\'.,;:'))\n",
    "    df['nouns'] = df['TRANSACTION DETAILS'].apply(lambda comment: tag_part_of_speech_noun(comment))\n",
    "    df['adjectives'] = df['TRANSACTION DETAILS'].apply(lambda comment: tag_part_of_speech_adj(comment))\n",
    "    df['verbs'] = df['TRANSACTION DETAILS'].apply(lambda comment: tag_part_of_speech_verb(comment))\n",
    "    df['nouns_vs_length'] = df['nouns'] / df['TRANSACTION_DETAILS_Length']\n",
    "    df['adjectives_vs_length'] = df['adjectives'] / df['TRANSACTION_DETAILS_Length']\n",
    "    df['verbs_vs_length'] = df['verbs'] / df['TRANSACTION_DETAILS_Length']\n",
    "    df['nouns_vs_words'] = df['nouns'] / df['Num_Words']\n",
    "    df['adjectives_vs_words'] = df['adjectives'] / df['Num_Words']\n",
    "    df['verbs_vs_words'] = df['verbs'] / df['Num_Words']\n",
    "    df[\"count_words_title\"] = df['TRANSACTION DETAILS'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    df[\"mean_word_len\"] = df['TRANSACTION DETAILS'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    df['punct_percent']= df['num_punctuation']*100 / df['Num_Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_ = CountVectorizer(min_df = 4, max_features = 5000, ngram_range = (3, 7))\n",
    "CV_.fit(train['TRANSACTION DETAILS'])\n",
    "\n",
    "train_words = pd.DataFrame(CV_.transform(train['TRANSACTION DETAILS']).toarray())\n",
    "test_words = pd.DataFrame(CV_.transform(test['TRANSACTION DETAILS']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components = 20, max_iter = 100, random_state = random_seed)\n",
    "LDA.fit(pd.concat([train_words,test_words]))\n",
    "Topics = [f'Topic_{x}' for x in range(0, 20)]\n",
    "train[Topics] = LDA.transform(train_words)\n",
    "test[Topics] = LDA.transform(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_top_words = 24\n",
    "# topic_summaries = []\n",
    "\n",
    "# # get topics and topic terms\n",
    "# topic_word = LDA.components_ \n",
    "# vocab = CV_.get_feature_names()\n",
    "\n",
    "# for i, topic_dist in enumerate(topic_word):\n",
    "#     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "#     topic_summaries.append(' '.join(topic_words))\n",
    "#     print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicated\n",
    "cols = train.columns\n",
    "dup = []\n",
    "for feat_1 in cols:\n",
    "    if (feat_1 in dup):\n",
    "        continue\n",
    "    for feat_2 in cols.drop(feat_1):\n",
    "        if (feat_2 in dup):\n",
    "            continue\n",
    "        if (train[feat_1].equals(train[feat_2])):\n",
    "            train.drop(feat_2,inplace=True,axis=1)\n",
    "            test.drop(feat_2,inplace=True,axis=1)\n",
    "            dup.append(feat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constant\n",
    "for feat in test.columns:\n",
    "    if ((len(train[feat].value_counts().keys()) == 1) | (len(test[feat].value_counts().keys()) == 1)):\n",
    "        train.drop(feat,inplace=True,axis=1)\n",
    "        test.drop(feat,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (56203, 303) (22625, 302)\n"
     ]
    }
   ],
   "source": [
    "# separating data into train and test\n",
    "# train = data[data.Category.notnull()].reset_index(drop = True)\n",
    "# test = data[data.Category.isna()].reset_index(drop = True)\n",
    "# test.drop('Category', axis = 1, inplace = True)\n",
    "print('shape', train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSACTION DETAILS\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "features = ['TRANSACTION DETAILS']\n",
    "full_data = pd.concat([train[features], test[features]], axis = 0)\n",
    "for col in (features):\n",
    "    le.fit(full_data[col].values)\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.drop('Category', axis = 1), train['Category']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set is missing cols {'Entertainment', 'Donations'}\n"
     ]
    }
   ],
   "source": [
    "# make sure validation set has all the categories as training set\n",
    "print('validation set is missing cols {}'.format(set(y_train.unique()) - set(y_val.unique())))\n",
    "missing_cols = list(set(y_train.unique()) - set(y_val.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sam, y_sam = RandomOverSampler(random_state = 42).fit_resample(X, y)\n",
    "# skf = StratifiedKFold(n_splits = 5, random_state = random_seed, shuffle = True)\n",
    "# X_Train, X_Val, y_Train, y_Val = train_test_split(X_sam, y_sam, test_size = 0.3, random_state = 121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_scores = []\n",
    "# for fold,(tr_in,te_in) in enumerate(skf.split(X, y)):\n",
    "#     print(\"===== Fold {fold} =====\".format(fold = fold + 1))\n",
    "#     X_train, X_test = X.iloc[tr_in], X.iloc[te_in]\n",
    "#     y_train, y_test = y.iloc[tr_in], y.iloc[te_in]\n",
    "#     cat = CatBoostClassifier(iterations = 1300, learning_rate = 0.09, depth = 6, reg_lambda = 17.56889442852513, verbose = 0, random_state = random_seed)\n",
    "#     cat.fit(X_train, y_train, eval_set = [(X_train, y_train),(X_test, y_test)], early_stopping_rounds = 500, use_best_model = True)\n",
    "#     loss = log_loss(y_test, cat.predict_proba(X_test))\n",
    "#     cat_scores.append(loss)\n",
    "#     print(loss)\n",
    "\n",
    "# print('cat_score : ' + str(np.mean(cat_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2001\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Training the model on the training dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      7\u001b[0m y_val_dummies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(pd\u001b[38;5;241m.\u001b[39mconcat([y_val, pd\u001b[38;5;241m.\u001b[39mSeries([col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m missing_cols])], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(missing_cols)][rf\u001b[38;5;241m.\u001b[39mclasses_]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 600, max_depth = 10, random_state = 2001)\n",
    "\n",
    "# Training the model on the training dataset\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_val)\n",
    "y_val_dummies = pd.get_dummies(pd.concat([y_val, pd.Series([col for col in missing_cols])], ignore_index=True)).iloc[:-len(missing_cols)][rf.classes_]\n",
    "y_pred_poba = rf.predict_proba(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(y_val, y_pred), 'The log loss of the model:',log_loss(y_val_dummies, y_pred_poba))\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier(l2_leaf_reg = 9.861413522475084, depth = 10, bootstrap_type = 'Bayesian', learning_rate = 0.02146, n_estimators = 3167,\n",
    "                         leaf_estimation_iterations = 1, random_strength = 0.18295032711212016, loss_function = 'MultiClass', verbose = 0, random_state = random_seed)\n",
    "\n",
    "# Training the model on the training dataset\n",
    "cat.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cat.predict(X_val)\n",
    "y_val_dummies = pd.get_dummies(pd.concat([y_val, pd.Series([col for col in missing_cols])], ignore_index=True)).iloc[:-len(missing_cols)][cat.classes_]\n",
    "y_pred_poba = cat.predict_proba(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(y_val, y_pred), 'The log loss of the model:',log_loss(y_val_dummies, y_pred_poba))\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(test)\n\u001b[0;32m      2\u001b[0m  \u001b[38;5;66;03m# * 0.4 + cat.predict_proba(test) * 0.6\u001b[39;00m\n\u001b[0;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(pred, columns \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mclasses_, index \u001b[38;5;241m=\u001b[39m sub\u001b[38;5;241m.\u001b[39mID)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf' is not defined"
     ]
    }
   ],
   "source": [
    "pred = rf.predict_proba(test)\n",
    " # * 0.4 + cat.predict_proba(test) * 0.6\n",
    "predictions = pd.DataFrame(pred, columns = rf.classes_, index = sub.ID)\n",
    "predictions.drop('Entertainment', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions.to_csv('first_2.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
